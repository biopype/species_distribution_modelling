# -*- coding: utf-8 -*-
"""Final SDM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11Shkll_2rqohG0mxbV9mt73YF7feTjzD

# Occurrence Data
"""

import pandas as pd
occurrence_data = pd.read_csv('/content/0031880-241126133413365.csv', sep='\t')

occurrence_data.columns

"""# Bioclimatic Variables"""

import zipfile
import os

zip_filename = '/content/wc2.1_10m_bio.zip'
extracted_folder = '/content/GeoTIFFs/'  # Folder to extract to

os.makedirs(extracted_folder, exist_ok=True)

with zipfile.ZipFile(zip_filename, 'r') as zip_ref:
    zip_ref.extractall(extracted_folder)

print("Files extracted:", os.listdir(extracted_folder))

"""# Data Preprocessing

Data Preprocessing for occurrence data
"""

import pandas as pd
import numpy as np

# 1. Drop unnecessary columns
columns_to_keep = ['gbifID', 'species', 'decimalLatitude', 'decimalLongitude', 'year', 'month', 'day', 'occurrenceStatus']
occurrence_data = occurrence_data[columns_to_keep]

# 2. Drop rows with missing coordinates
occurrence_data.dropna(subset=['decimalLatitude', 'decimalLongitude'], inplace=True)

# Drop rows with missing species information
occurrence_data.dropna(subset=['species'], inplace=True)

# 3. Remove duplicates
occurrence_data.drop_duplicates(inplace=True)

# 4. Validate coordinate ranges
occurrence_data = occurrence_data[
    (occurrence_data['decimalLatitude'].between(-90, 90)) &
    (occurrence_data['decimalLongitude'].between(-180, 180))
]

# 5. Standardize species names (e.g., remove trailing/leading whitespaces)
occurrence_data['species'] = occurrence_data['species'].str.strip()

# 6. Save the cleaned data to a new CSV
cleaned_file_path = 'cleaned_species_occurrence.csv'
occurrence_data.to_csv(cleaned_file_path, index=False)

"""Natural Earth Data for plotting"""

import zipfile
import os

zip_file_path = '/content/ne_10m_admin_0_countries.zip'
extract_dir = '/content/NED'

os.makedirs(extract_dir, exist_ok=True)

# Extract the ZIP file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

import geopandas as gpd
import matplotlib.pyplot as plt
from shapely.geometry import Point

# Load the cleaned occurrence data
file_path = 'cleaned_species_occurrence.csv'
occurrence_data = pd.read_csv(file_path)

# Create a GeoDataFrame for visualization
geometry = [Point(xy) for xy in zip(occurrence_data['decimalLongitude'], occurrence_data['decimalLatitude'])]
gdf = gpd.GeoDataFrame(occurrence_data, geometry=geometry)

# Load the Natural Earth dataset
world_shapefile_path = '/content/NED/ne_10m_admin_0_countries.shx'  # (.shx) natural earth data obtained from https://www.naturalearthdata.com/downloads/10m-cultural-vectors/
world = gpd.read_file(world_shapefile_path)

# Plot the data
fig, ax = plt.subplots(figsize=(10, 6))
world.plot(ax=ax, color='lightgray', edgecolor='black')
gdf.plot(ax=ax, markersize=10, color='blue', alpha=0.6, label='Occurrences')

plt.title("Species Occurrence Distribution")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.legend()
plt.show()

!pip install rasterio

import os
import rasterio
import pandas as pd
import numpy as np

# Path to the folder containing the GeoTIFF files
geotiff_folder = "GeoTIFFs"

# List all GeoTIFF files in the folder
bioclim_files = [os.path.join(geotiff_folder, f) for f in os.listdir(geotiff_folder) if f.endswith('.tif')]

# Load the cleaned occurrence data
file_path = 'cleaned_species_occurrence.csv'
occurrence_data = pd.read_csv(file_path)

# Extract coordinates of occurrence points
points = [(lon, lat) for lon, lat in zip(occurrence_data['decimalLongitude'], occurrence_data['decimalLatitude'])]

# Function to extract bioclimatic data from GeoTIFFs at occurrence locations
def extract_bioclim_data(bioclim_files, points):
    bioclim_data = []
    for file in bioclim_files:
        with rasterio.open(file) as src:
            variable_name = os.path.basename(file)[0]
            values = []
            for lon, lat in points:
                try:
                    row, col = src.index(lon, lat)  # Get raster indices
                    value = src.read(1)[row, col]  # Read the value
                    if value != src.nodata:  # Exclude nodata values
                        values.append(value)
                    else:
                        values.append(np.nan)
                except IndexError:
                    # Assign NaN to points outside raster bounds
                    values.append(np.nan)
            bioclim_data.append(values)
    return bioclim_data

"""With the longitude and latitude present in the occurrence dataset, corresponding values using all 19 variables from the tif files are calculated.

"""

import os
import rasterio
import pandas as pd
import numpy as np

# Load the cleaned occurrence data and bioclimatic data using the shared function
bioclim_data = extract_bioclim_data(bioclim_files, [(lon, lat) for lon, lat in zip(occurrence_data['decimalLongitude'], occurrence_data['decimalLatitude'])])

# Initialize an empty list to store summary statistics
summary_stats_list = []

# Calculate summary statistics for each variable
for i, file in enumerate(bioclim_files):
    variable_name = os.path.basename(file)[0]
    values = np.array(bioclim_data[i])

    # Remove missing values (NaNs)
    values = values[~np.isnan(values)]

    if values.size > 0:
        summary_stats_list.append({
            'Variable': variable_name,
            'Mean': np.mean(values),
            'Min': np.min(values),
            'Max': np.max(values),
            'Std': np.std(values)
        })

# Convert the list of summary statistics to a DataFrame
summary_stats = pd.DataFrame(summary_stats_list)

# Save the summary statistics to a CSV file
summary_stats.to_csv('environmental_summary_statistics.csv', index=False)
print("Summary statistics saved to 'environmental_summary_statistics.csv'")

summary_stats

"""# Correlation analysis and multicollinearity"""

import os
import pandas as pd
import numpy as np
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.preprocessing import StandardScaler

# Load the cleaned occurrence data and bioclimatic data using the shared function
bioclim_data = extract_bioclim_data(bioclim_files, [(lon, lat) for lon, lat in zip(occurrence_data['decimalLongitude'], occurrence_data['decimalLatitude'])])

# Convert to DataFrame
bioclim_df = pd.DataFrame(np.array(bioclim_data).T, columns=[f"Bioclim_{i}" for i in range(1, 20)])

# Drop rows with missing data
bioclim_df.dropna(inplace=True)

# Correlation matrix
correlation_matrix = bioclim_df.corr()

# Calculate Variance Inflation Factor (VIF)
scaler = StandardScaler()
bioclim_scaled = scaler.fit_transform(bioclim_df)

vif_data = pd.DataFrame()
vif_data['Variable'] = bioclim_df.columns
vif_data['VIF'] = [variance_inflation_factor(bioclim_scaled, i) for i in range(bioclim_scaled.shape[1])]

# Select key variables with VIF < 10
key_variables = vif_data[vif_data['VIF'] < 10]['Variable']

correlation_matrix

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.DataFrame(correlation_matrix)

# Plot the heatmap
plt.figure(figsize=(18, 16))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title("Correlation Matrix")
plt.show()

print("\nVariance Inflation Factor:")
vif_data

import matplotlib.pyplot as plt
import seaborn as sns

# Visualize VIF values
plt.figure(figsize=(8, 6))
sns.barplot(x='VIF', y='Variable', data=vif_data, palette='viridis')
plt.title('Variance Inflation Factor (VIF)')
plt.xlabel('VIF Value')
plt.ylabel('Features')
plt.axvline(5, color='red', linestyle='--', label='VIF = 5 (Threshold)')
plt.legend()
plt.show()

print("\nKey Variables:")
key_variables

"""# Model Development"""

print(occurrence_data.columns)

bioclim_df = bioclim_df.drop ('Bioclim_14', axis=1)

bioclim_df

occurrence_data.to_csv('occurrence_data.csv')

bioclim_df.to_csv('bioclim_df.csv')

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Convert 'occurrenceStatus' to binary format
occurrence_data['occurrenceStatus'] = occurrence_data['occurrenceStatus'].map({'PRESENT': 1, 'ABSENT':0})

# Combine bioclimatic data with occurrenceStatus
data = pd.concat([bioclim_df, occurrence_data['occurrenceStatus']], axis=1)

data

data.dropna()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import joblib

# Drop rows with missing target values
data.dropna(inplace=True)

X = data.drop('occurrenceStatus', axis=1)
y = data['occurrenceStatus']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Train Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions
y_pred = rf_model.predict(X_test)
y_prob = rf_model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Print classification report and confusion matrix
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Compute AUC-ROC
auc_score = roc_auc_score(y_test, y_prob)
print("\nAUC-ROC Score:", auc_score)

# Plot the ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f"AUC = {auc_score:.2f}", color='darkorange')
plt.plot([0, 1], [0, 1], 'k--', label="Random Classifier")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend(loc="lower right")
plt.grid()
plt.show()

# Save the model
model_path = 'random_forest_model.joblib'
joblib.dump(rf_model, model_path)
print(f"Model saved to {model_path}")

import numpy as np
import pandas as pd
import rasterio
import matplotlib.pyplot as plt
import geopandas as gpd
from shapely.geometry import Point
from sklearn.ensemble import RandomForestClassifier
from joblib import load

# Load the trained model
model_path = '/content/random_forest_model.joblib'
loaded_model = load(model_path)

# Load environmental rasters (bioclim variables)
raster_files = [
    '/content/GeoTIFFs/wc2.1_10m_bio_1.tif',
    '/content/GeoTIFFs/wc2.1_10m_bio_2.tif',
    '/content/GeoTIFFs/wc2.1_10m_bio_3.tif',
    '/content/GeoTIFFs/wc2.1_10m_bio_4.tif',
    '/content/GeoTIFFs/wc2.1_10m_bio_5.tif',
    '/content/GeoTIFFs/wc2.1_10m_bio_6.tif',
    '/content/GeoTIFFs/wc2.1_10m_bio_7.tif',
    '/content/GeoTIFFs/wc2.1_10m_bio_8.tif',
    '/content/GeoTIFFs/wc2.1_10m_bio_9.tif',
    '/content/GeoTIFFs/wc2.1_10m_bio_10.tif',
    '/content/GeoTIFFs/wc2.1_10m_bio_11.tif',
    '/content/GeoTIFFs/wc2.1_10m_bio_12.tif',
    '/content/GeoTIFFs/wc2.1_10m_bio_13.tif',
    '/content/GeoTIFFs/wc2.1_10m_bio_15.tif',
    '/content/GeoTIFFs/wc2.1_10m_bio_16.tif',
    '/content/GeoTIFFs/wc2.1_10m_bio_17.tif',
    '/content/GeoTIFFs/wc2.1_10m_bio_18.tif',
    '/content/GeoTIFFs/wc2.1_10m_bio_19.tif',
]

# Read rasters and stack them into a 3D array
rasters = []
transform = None
crs = None

for file in raster_files:
    with rasterio.open(file) as src:
        rasters.append(src.read(1))  # Read the first band
        if transform is None:
            transform = src.transform
            crs = src.crs

# Ensure all rasters are the same shape
raster_stack = np.stack(rasters, axis=-1)  # Shape: (rows, cols, n_variables)

# Flatten raster stack for prediction
n_rows, n_cols, n_features = raster_stack.shape
flattened_features = raster_stack.reshape(-1, n_features)

# Predict habitat suitability
suitability_scores = loaded_model.predict_proba(flattened_features)[:, 1]  # Probability of occurrence
suitability_map = suitability_scores.reshape(n_rows, n_cols)

# Step 1: Save the Habitat Suitability Map to a GeoTIFF
output_raster_path = '/content/habitat_suitability_map.tif'

with rasterio.open(
    output_raster_path, 'w', driver='GTiff', count=1, dtype='float32',
    height=n_rows, width=n_cols, crs=crs, transform=transform
) as dst:
    dst.write(suitability_map, 1)

print(f"Habitat suitability map saved to {output_raster_path}")

# Step 2: Visualize the Habitat Suitability Map using Matplotlib
plt.figure(figsize=(12, 8))
plt.imshow(suitability_map, cmap='viridis', interpolation='nearest', extent=(
    transform[2], transform[2] + transform[0] * n_cols,
    transform[5] + transform[4] * n_rows, transform[5]
))
plt.colorbar(label='Suitability Score')
plt.title('Habitat Suitability Map for Capra falconeri')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.show()

# Step 3: Overlay Occurrence Points for Validation
occurrence_points_csv = '/content/occurrence_data.csv'
occurrence_data = pd.read_csv(occurrence_points_csv)
gdf = gpd.GeoDataFrame(
    occurrence_data,
    geometry=gpd.points_from_xy(occurrence_data['decimalLongitude'], occurrence_data['decimalLatitude'])
)

# Plot suitability map with occurrence points
fig, ax = plt.subplots(figsize=(12, 8))
plt.imshow(suitability_map, cmap='viridis', interpolation='nearest', extent=(
    transform[2], transform[2] + transform[0] * n_cols,
    transform[5] + transform[4] * n_rows, transform[5]
))
plt.colorbar(label='Suitability Score')
plt.title('Habitat Suitability Map with Occurrence Points')
plt.xlabel('Longitude')
plt.ylabel('Latitude')

# Plot occurrence points
gdf.plot(ax=ax, color='red', markersize=5, label='Occurrence Points', alpha=0.6)
plt.legend()
plt.show()